{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokemon Winner Classification\n",
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"input_data\", \"pokemon_combats.csv\")\n",
    "table = MyPyTable().load_from_file(filename)\n",
    "\n",
    "# pokemon type\n",
    "type_1_first = table.get_column(\"Type 1_first\")\n",
    "type_2_first = table.get_column(\"Type 2_first\")\n",
    "type_1_second = table.get_column(\"Type 1_second\")\n",
    "type_2_second = table.get_column(\"Type 2_second\")\n",
    "X_types = [[t1f, t2f, t1s, t2s] for t1f, t2f, t1s, t2s in zip(type_1_first, type_2_first, type_1_second, type_2_second)]\n",
    "y = table.get_column(\"Winner\")\n",
    "train_sets_types, test_sets_types = myevaluation.stratified_kfold_cross_validation(X_types, y, n_splits=10, random_state=0, shuffle=False)\n",
    "\n",
    "# # game statistics\n",
    "# field_goals = table.get_column(\"RegularSeasonFGPercentMean\")\n",
    "# three_ptrs = table.get_column(\"RegularSeasonFG3PercentMean\")\n",
    "# turn_overs = table.get_column(\"RegularSeasonTOMean\")\n",
    "# steals = table.get_column(\"RegularSeasonStlMean\")\n",
    "# X_stats = [[fg, tp, to, s] for fg, tp, to, s in zip(field_goals, three_ptrs, turn_overs, steals)]\n",
    "# train_sets_stats, test_sets_stats = myevaluation.stratified_kfold_cross_validation(X_stats, y, n_splits=10, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Winners Using Pokemon Type\n",
    "Pokemon is essentially more complicated game of rock, paper, scissors in that some pokemon have type advantages that make them strong or resistant against other types. For instance, Water types are strong against Fire types, but weak against Grass types. Thus, the best predictor of who won a battle would likely be Pokemon typings. Each Pokemon can have up to two different types of the following types: Poison, Flying, Dragon, Ground, Fairy, Grass, Fighting, Psychic, Steel, Ice, Rock, Dark, Water, Electric, Fire, Ghost, Bug, Normal.\n",
    "\n",
    "To test how well typing serves as a predictor, kNN, Naive Bayes, Dummy, and Decision Tree classifiers made predictions for each test set in each fold. All predictions are stored for performance scoring against `y_true`, the correct classifier for each test instance. Several metrics for performance were compared among the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = MyKNeighborsClassifier(n_neighbors=3)\n",
    "nb = MyNaiveBayesClassifier()\n",
    "dummy = MyDummyClassifier()\n",
    "dt = MyDecisionTreeClassifier()\n",
    "\n",
    "# accuracy is the total correctly predicted divided by the total predicted over all the folds\n",
    "knn_correct, nb_correct, dummy_correct, dt_correct, total_predicted = 0, 0, 0, 0, 0\n",
    "knn_preds, nb_preds, dummy_preds, dt_preds, y_true = [], [], [], [], []\n",
    "\n",
    "for train, test in zip(train_sets_types, test_sets_types):\n",
    "    X_train = [X_types[i] for i in train]\n",
    "    y_train = [y[i] for i in train]\n",
    "    X_test = [X_types[i] for i in test]\n",
    "    y_test = [y[i] for i in test]\n",
    "    total_predicted += len(y_test)\n",
    "    y_true += y_test\n",
    "\n",
    "    # kNN\n",
    "    # print(X_train)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    knn_preds += knn_pred\n",
    "    knn_correct += myevaluation.accuracy_score(y_test, knn_pred, normalize=False)\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_preds += nb_pred\n",
    "    nb_correct += myevaluation.accuracy_score(y_test, nb_pred, normalize=False)\n",
    "\n",
    "    # Dummy\n",
    "    dummy.fit(X_train, y_train)\n",
    "    dummy_pred = dummy.predict(X_test)\n",
    "    dummy_preds += dummy_pred\n",
    "    dummy_correct += myevaluation.accuracy_score(y_test, dummy_pred, normalize=False)\n",
    "\n",
    "    # Decision Tree\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_pred = dt.predict(X_test)\n",
    "    dt_preds += dt_pred\n",
    "    dt_correct += myevaluation.accuracy_score(y_test, dt_preds, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Performance Using Pokemon Type\n",
    "How well can typing predict the winning Pokemon?\n",
    "\n",
    "### kNN\n",
    "kNN had a relatively low accuracy for a binary class prediction with 48.2% accuracy. kNN seemed to predict \"A\" almost every time, leading to an extremely low recall score of 7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "kNN Classifier\n",
      "===========================================\n",
      "accuracy score: 0.482\n",
      "error rate: 0.518\n",
      "precision score: 0.542\n",
      "recall score: 0.074\n",
      "f1 score: 0.131\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H          13  162      175              7.429\n",
      "A          11  148      159             93.082\n"
     ]
    }
   ],
   "source": [
    "labels = [1, 2]\n",
    "pos_label = 1\n",
    "\n",
    "knn_accuracy = knn_correct / total_predicted\n",
    "knn_precision = myevaluation.binary_precision_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_recall = myevaluation.binary_recall_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_f1 = myevaluation.binary_f1_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_confusion_matrix = myevaluation.confusion_matrix(y_true, knn_preds, labels)\n",
    "\n",
    "myutils.print_results(\"kNN Classifier\", knn_accuracy, knn_precision, knn_recall, knn_f1)\n",
    "header = [\"Winner\", \"H\", \"A\"]\n",
    "myutils.print_confusion_matrix(header, knn_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive Bayes performed the best of the classifiers with an accuracy of 68.9%. Precision, recall, and f1 scores were also high (>68%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Naive Bayes Classifier\n",
      "===========================================\n",
      "accuracy score: 0.689\n",
      "error rate: 0.311\n",
      "precision score: 0.682\n",
      "recall score: 0.76\n",
      "f1 score: 0.719\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         133   42      175             76\n",
      "A          62   97      159             61.006\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy = nb_correct / total_predicted\n",
    "nb_precision = myevaluation.binary_precision_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_recall = myevaluation.binary_recall_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_f1 = myevaluation.binary_f1_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_confusion_matrix = myevaluation.confusion_matrix(y_true, nb_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Naive Bayes Classifier\", nb_accuracy, nb_precision, nb_recall, nb_f1)\n",
    "myutils.print_confusion_matrix(header, nb_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy\n",
    "Dummy performed with 47.3% accuracy, same as kNN. Because Dummy uses the majority class label and classifications were binary, Dummy had an excellent recall score of 85.7%, but a terribly low precision score of ~5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Dummy Classifier\n",
      "===========================================\n",
      "accuracy score: 0.473\n",
      "error rate: 0.527\n",
      "precision score: 0.498\n",
      "recall score: 0.857\n",
      "f1 score: 0.63\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         150   25      175             85.714\n",
      "A         151    8      159              5.031\n"
     ]
    }
   ],
   "source": [
    "dummy_accuracy = dummy_correct / total_predicted\n",
    "dummy_precision = myevaluation.binary_precision_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_recall = myevaluation.binary_recall_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_f1 = myevaluation.binary_f1_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_confusion_matrix = myevaluation.confusion_matrix(y_true, dummy_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Dummy Classifier\", dummy_accuracy, dummy_precision, dummy_recall, dummy_f1)\n",
    "myutils.print_confusion_matrix(header, dummy_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Decision tree performed with 52.4% accuracy rate and relatively low precision, recall, and f1 scores (~32%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Decision Tree Classifier\n",
      "===========================================\n",
      "accuracy score: 0.524\n",
      "error rate: 0.476\n",
      "precision score: 0.337\n",
      "recall score: 0.314\n",
      "f1 score: 0.325\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H          55  120      175             31.429\n",
      "A         108   51      159             32.075\n"
     ]
    }
   ],
   "source": [
    "dt_accuracy = dt_correct / total_predicted\n",
    "dt_precision = myevaluation.binary_precision_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_recall = myevaluation.binary_recall_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_f1 = myevaluation.binary_f1_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_confusion_matrix = myevaluation.confusion_matrix(y_true, dt_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Decision Tree Classifier\", dt_accuracy, dt_precision, dt_recall, dt_f1)\n",
    "myutils.print_confusion_matrix(header, dt_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Winner Using Pokemon Stats\n",
    "kNN, Naive Bayes, Dummy, and Decision Tree classifiers make predictions for each test set in each fold. All predictions are stored for performance scoring against `y_true`, the correct classifier for each test instance. Attributes used for predictions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy is the total correctly predicted divided by the total predicted over all the folds\n",
    "knn_correct, nb_correct, dummy_correct, dt_correct, total_predicted = 0, 0, 0, 0, 0\n",
    "knn_preds, nb_preds, dummy_preds, dt_preds, y_true = [], [], [], [], []\n",
    "\n",
    "for train, test in zip(train_sets_stats, test_sets_stats):\n",
    "    X_train = [X_stats[i] for i in train]\n",
    "    y_train = [y_stats[i] for i in train]\n",
    "    X_test = [X_stats[i] for i in test]\n",
    "    y_test = [y_stats[i] for i in test]\n",
    "    total_predicted += len(y_test)\n",
    "    y_true += y_test\n",
    "\n",
    "    # kNN\n",
    "    # print(X_train)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    knn_preds += knn_pred\n",
    "    knn_correct += myevaluation.accuracy_score(y_test, knn_pred, normalize=False)\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_preds += nb_pred\n",
    "    nb_correct += myevaluation.accuracy_score(y_test, nb_pred, normalize=False)\n",
    "\n",
    "    # Dummy\n",
    "    dummy.fit(X_train, y_train)\n",
    "    dummy_pred = dummy.predict(X_test)\n",
    "    dummy_preds += dummy_pred\n",
    "    dummy_correct += myevaluation.accuracy_score(y_test, dummy_pred, normalize=False)\n",
    "\n",
    "    # Decision Tree\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_pred = dt.predict(X_test)\n",
    "    dt_preds += dt_pred\n",
    "    dt_correct += myevaluation.accuracy_score(y_test, dt_preds, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Performance Using Pokemon Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN\n",
    "kNN scored an accuracy of 53% and slightly lower precision, recall, and f1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "kNN Classifier\n",
      "===========================================\n",
      "accuracy score: 0.53\n",
      "error rate: 0.47\n",
      "precision score: 0.567\n",
      "recall score: 0.434\n",
      "f1 score: 0.492\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H          76   99      175             43.429\n",
      "A          58  101      159             63.522\n"
     ]
    }
   ],
   "source": [
    "labels = [\"H\", \"A\"]\n",
    "pos_label = \"H\"\n",
    "\n",
    "knn_accuracy = knn_correct / total_predicted\n",
    "knn_precision = myevaluation.binary_precision_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_recall = myevaluation.binary_recall_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_f1 = myevaluation.binary_f1_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_confusion_matrix = myevaluation.confusion_matrix(y_true, knn_preds, labels)\n",
    "\n",
    "myutils.print_results(\"kNN Classifier\", knn_accuracy, knn_precision, knn_recall, knn_f1)\n",
    "header = [\"Winner\", \"H\", \"A\"]\n",
    "myutils.print_confusion_matrix(header, knn_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Once again, Naive Bayes performed the best of the classifiers with an accuracy of 61%. Precision, recall, and f1 were all also in the range of 60-65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Naive Bayes Classifier\n",
      "===========================================\n",
      "accuracy score: 0.587\n",
      "error rate: 0.413\n",
      "precision score: 0.603\n",
      "recall score: 0.617\n",
      "f1 score: 0.61\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         108   67      175             61.714\n",
      "A          71   88      159             55.346\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy = nb_correct / total_predicted\n",
    "nb_precision = myevaluation.binary_precision_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_recall = myevaluation.binary_recall_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_f1 = myevaluation.binary_f1_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_confusion_matrix = myevaluation.confusion_matrix(y_true, nb_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Naive Bayes Classifier\", nb_accuracy, nb_precision, nb_recall, nb_f1)\n",
    "myutils.print_confusion_matrix(header, nb_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy\n",
    "Dummy performed with an accuracy of 47.3%. Due to the mechanics of Dummy, all performance stats are the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Dummy Classifier\n",
      "===========================================\n",
      "accuracy score: 0.473\n",
      "error rate: 0.527\n",
      "precision score: 0.498\n",
      "recall score: 0.857\n",
      "f1 score: 0.63\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         150   25      175             85.714\n",
      "A         151    8      159              5.031\n"
     ]
    }
   ],
   "source": [
    "dummy_accuracy = dummy_correct / total_predicted\n",
    "dummy_precision = myevaluation.binary_precision_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_recall = myevaluation.binary_recall_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_f1 = myevaluation.binary_f1_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_confusion_matrix = myevaluation.confusion_matrix(y_true, dummy_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Dummy Classifier\", dummy_accuracy, dummy_precision, dummy_recall, dummy_f1)\n",
    "myutils.print_confusion_matrix(header, dummy_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Decision tree surprisingly performed the worst, with an accuracy of 45.8%. This is likely due to overfitting. Pruning the decision tree could help performance (see below for pruning analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Decision Tree Classifier\n",
      "===========================================\n",
      "accuracy score: 0.458\n",
      "error rate: 0.542\n",
      "precision score: 0.49\n",
      "recall score: 0.44\n",
      "f1 score: 0.464\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H          77   98      175             44\n",
      "A          80   79      159             49.686\n"
     ]
    }
   ],
   "source": [
    "dt_accuracy = dt_correct / total_predicted\n",
    "dt_precision = myevaluation.binary_precision_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_recall = myevaluation.binary_recall_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_f1 = myevaluation.binary_f1_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_confusion_matrix = myevaluation.confusion_matrix(y_true, dt_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Decision Tree Classifier\", dt_accuracy, dt_precision, dt_recall, dt_f1)\n",
    "myutils.print_confusion_matrix(header, dt_confusion_matrix, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
