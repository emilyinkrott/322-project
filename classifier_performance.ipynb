{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokemon Winner Classification\n",
    "\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful mysklearn package import statements and reloads\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "combats_filename = os.path.join(\"input_data\", \"combats.csv\")\n",
    "pokemon_filename = os.path.join(\"input_data\", \"pokemon.csv\")\n",
    "\n",
    "combats = pd.read_csv(combats_filename)\n",
    "pokemon = pd.read_csv(pokemon_filename)\n",
    "\n",
    "table = pd.merge(combats, pokemon, left_on=['First_pokemon'], right_on=['#'])\n",
    "new_table = pd.merge(table, pokemon, left_on=['Second_pokemon'], right_on=['#'], suffixes=(\"_first\", \"_second\"))\n",
    "filename = os.path.join(\"input_data\", \"pokemon_combats.csv\")\n",
    "new_table.to_csv(filename, index=False)\n",
    "\n",
    "final_table = MyPyTable().load_from_file(filename)\n",
    "for i, row in enumerate(final_table.data):\n",
    "    if row[-1] == row[0]:\n",
    "        final_table.data[i][-1] = 1\n",
    "    else:\n",
    "        final_table.data[i][-1] = 2\n",
    "\n",
    "filename_1 = os.path.join(\"input_data\", \"pokemon_combats_1.csv\")\n",
    "final_table.save_to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"input_data\", \"tournament_games2016-2021.csv\")\n",
    "table = MyPyTable().load_from_file(filename)\n",
    "\n",
    "# tournament seed\n",
    "seeds = table.get_column(\"TournamentSeed\")\n",
    "X_seeds = [[seed] for seed in seeds]\n",
    "y_seeds = table.get_column(\"Winner\")\n",
    "train_sets_seeds, test_sets_seeds = myevaluation.stratified_kfold_cross_validation(X_seeds, y_seeds, n_splits=10, random_state=0, shuffle=False)\n",
    "\n",
    "# game statistics\n",
    "field_goals = table.get_column(\"RegularSeasonFGPercentMean\")\n",
    "three_ptrs = table.get_column(\"RegularSeasonFG3PercentMean\")\n",
    "turn_overs = table.get_column(\"RegularSeasonTOMean\")\n",
    "steals = table.get_column(\"RegularSeasonStlMean\")\n",
    "X_stats = [[fg, tp, to, s] for fg, tp, to, s in zip(field_goals, three_ptrs, turn_overs, steals)]\n",
    "y_stats = y_seeds\n",
    "train_sets_stats, test_sets_stats = myevaluation.stratified_kfold_cross_validation(X_stats, y_stats, n_splits=10, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Winners Using Tournament Seed\n",
    "kNN, Naive Bayes, Dummy, and Decision Tree classifiers make predictions for each test set in each fold. All predictions are stored for performance scoring against `y_true`, the correct classifier for each test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = MyKNeighborsClassifier(n_neighbors=10)\n",
    "nb = MyNaiveBayesClassifier()\n",
    "dummy = MyDummyClassifier()\n",
    "dt = MyDecisionTreeClassifier()\n",
    "\n",
    "# accuracy is the total correctly predicted divided by the total predicted over all the folds\n",
    "knn_correct, nb_correct, dummy_correct, dt_correct, total_predicted = 0, 0, 0, 0, 0\n",
    "knn_preds, nb_preds, dummy_preds, dt_preds, y_true = [], [], [], [], []\n",
    "\n",
    "for train, test in zip(train_sets_seeds, test_sets_seeds):\n",
    "    X_train = [X_seeds[i] for i in train]\n",
    "    y_train = [y_seeds[i] for i in train]\n",
    "    X_test = [X_seeds[i] for i in test]\n",
    "    y_test = [y_seeds[i] for i in test]\n",
    "    total_predicted += len(y_test)\n",
    "    y_true += y_test\n",
    "\n",
    "    # kNN\n",
    "    # print(X_train)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    knn_preds += knn_pred\n",
    "    knn_correct += myevaluation.accuracy_score(y_test, knn_pred, normalize=False)\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_preds += nb_pred\n",
    "    nb_correct += myevaluation.accuracy_score(y_test, nb_pred, normalize=False)\n",
    "\n",
    "    # Dummy\n",
    "    dummy.fit(X_train, y_train)\n",
    "    dummy_pred = dummy.predict(X_test)\n",
    "    dummy_preds += dummy_pred\n",
    "    dummy_correct += myevaluation.accuracy_score(y_test, dummy_pred, normalize=False)\n",
    "\n",
    "    # Decision Tree\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_pred = dt.predict(X_test)\n",
    "    dt_preds += dt_pred\n",
    "    dt_correct += myevaluation.accuracy_score(y_test, dt_preds, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Performance Using Tournament Seed\n",
    "How well can tournament seed predict the winning team?\n",
    "\n",
    "### kNN\n",
    "kNN had a relatively low accuracy for a binary class prediction with 48.2% accuracy. kNN seemed to predict \"A\" almost every time, leading to an extremely low recall score of 7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "kNN Classifier\n",
      "===========================================\n",
      "accuracy score: 0.482\n",
      "error rate: 0.518\n",
      "precision score: 0.542\n",
      "recall score: 0.074\n",
      "f1 score: 0.131\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H          13  162      175              7.429\n",
      "A          11  148      159             93.082\n"
     ]
    }
   ],
   "source": [
    "labels = [\"H\", \"A\"]\n",
    "pos_label = \"H\"\n",
    "\n",
    "knn_accuracy = knn_correct / total_predicted\n",
    "knn_precision = myevaluation.binary_precision_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_recall = myevaluation.binary_recall_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_f1 = myevaluation.binary_f1_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_confusion_matrix = myevaluation.confusion_matrix(y_true, knn_preds, labels)\n",
    "\n",
    "myutils.print_results(\"kNN Classifier\", knn_accuracy, knn_precision, knn_recall, knn_f1)\n",
    "header = [\"Winner\", \"H\", \"A\"]\n",
    "myutils.print_confusion_matrix(header, knn_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive Bayes performed the best of the classifiers with an accuracy of 68.9%. Precision, recall, and f1 scores were also high (>68%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Naive Bayes Classifier\n",
      "===========================================\n",
      "accuracy score: 0.689\n",
      "error rate: 0.311\n",
      "precision score: 0.682\n",
      "recall score: 0.76\n",
      "f1 score: 0.719\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         133   42      175             76\n",
      "A          62   97      159             61.006\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy = nb_correct / total_predicted\n",
    "nb_precision = myevaluation.binary_precision_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_recall = myevaluation.binary_recall_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_f1 = myevaluation.binary_f1_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_confusion_matrix = myevaluation.confusion_matrix(y_true, nb_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Naive Bayes Classifier\", nb_accuracy, nb_precision, nb_recall, nb_f1)\n",
    "myutils.print_confusion_matrix(header, nb_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy\n",
    "Dummy performed with 47.3% accuracy, same as kNN. Because Dummy uses the majority class label and classifications were binary, Dummy had an excellent recall score of 85.7%, but a terribly low precision score of ~5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Dummy Classifier\n",
      "===========================================\n",
      "accuracy score: 0.473\n",
      "error rate: 0.527\n",
      "precision score: 0.498\n",
      "recall score: 0.857\n",
      "f1 score: 0.63\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         150   25      175             85.714\n",
      "A         151    8      159              5.031\n"
     ]
    }
   ],
   "source": [
    "dummy_accuracy = dummy_correct / total_predicted\n",
    "dummy_precision = myevaluation.binary_precision_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_recall = myevaluation.binary_recall_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_f1 = myevaluation.binary_f1_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_confusion_matrix = myevaluation.confusion_matrix(y_true, dummy_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Dummy Classifier\", dummy_accuracy, dummy_precision, dummy_recall, dummy_f1)\n",
    "myutils.print_confusion_matrix(header, dummy_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Decision tree performed with 52.4% accuracy rate and relatively low precision, recall, and f1 scores (~32%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Decision Tree Classifier\n",
      "===========================================\n",
      "accuracy score: 0.524\n",
      "error rate: 0.476\n",
      "precision score: 0.337\n",
      "recall score: 0.314\n",
      "f1 score: 0.325\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H          55  120      175             31.429\n",
      "A         108   51      159             32.075\n"
     ]
    }
   ],
   "source": [
    "dt_accuracy = dt_correct / total_predicted\n",
    "dt_precision = myevaluation.binary_precision_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_recall = myevaluation.binary_recall_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_f1 = myevaluation.binary_f1_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_confusion_matrix = myevaluation.confusion_matrix(y_true, dt_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Decision Tree Classifier\", dt_accuracy, dt_precision, dt_recall, dt_f1)\n",
    "myutils.print_confusion_matrix(header, dt_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Winner Using Game Statistics\n",
    "kNN, Naive Bayes, Dummy, and Decision Tree classifiers make predictions for each test set in each fold. All predictions are stored for performance scoring against `y_true`, the correct classifier for each test instance. Attributes used for predictions are:\n",
    "* RegularSeasonFGPercentMean: which team (\"H\" or \"A\") has the numerically higher field goal percentage during this tournament game's corresponding regular season\n",
    "* RegularSeasonFG3PercentMean: which team (\"H\" or \"A\") has the numerically higher 3-pointer percentage during this tournament game's corresponding regular season\n",
    "* RegularSeasonTOMean: which team (\"H\" or \"A\") has the numerically higher turnover percentage during this tournament game's corresponding regular season\n",
    "* RegularSeasonStlMean: which team (\"H\" or \"A\") has the numerically higher accomplished steals percentage during this tournament game's corresponding regular season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy is the total correctly predicted divided by the total predicted over all the folds\n",
    "knn_correct, nb_correct, dummy_correct, dt_correct, total_predicted = 0, 0, 0, 0, 0\n",
    "knn_preds, nb_preds, dummy_preds, dt_preds, y_true = [], [], [], [], []\n",
    "\n",
    "for train, test in zip(train_sets_stats, test_sets_stats):\n",
    "    X_train = [X_stats[i] for i in train]\n",
    "    y_train = [y_stats[i] for i in train]\n",
    "    X_test = [X_stats[i] for i in test]\n",
    "    y_test = [y_stats[i] for i in test]\n",
    "    total_predicted += len(y_test)\n",
    "    y_true += y_test\n",
    "\n",
    "    # kNN\n",
    "    # print(X_train)\n",
    "    knn.fit(X_train, y_train)\n",
    "    knn_pred = knn.predict(X_test)\n",
    "    knn_preds += knn_pred\n",
    "    knn_correct += myevaluation.accuracy_score(y_test, knn_pred, normalize=False)\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb.fit(X_train, y_train)\n",
    "    nb_pred = nb.predict(X_test)\n",
    "    nb_preds += nb_pred\n",
    "    nb_correct += myevaluation.accuracy_score(y_test, nb_pred, normalize=False)\n",
    "\n",
    "    # Dummy\n",
    "    dummy.fit(X_train, y_train)\n",
    "    dummy_pred = dummy.predict(X_test)\n",
    "    dummy_preds += dummy_pred\n",
    "    dummy_correct += myevaluation.accuracy_score(y_test, dummy_pred, normalize=False)\n",
    "\n",
    "    # Decision Tree\n",
    "    dt.fit(X_train, y_train)\n",
    "    dt_pred = dt.predict(X_test)\n",
    "    dt_preds += dt_pred\n",
    "    dt_correct += myevaluation.accuracy_score(y_test, dt_preds, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Performance Using Game Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN\n",
    "kNN scored an accuracy of 53% and slightly lower precision, recall, and f1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "kNN Classifier\n",
      "===========================================\n",
      "accuracy score: 0.53\n",
      "error rate: 0.47\n",
      "precision score: 0.567\n",
      "recall score: 0.434\n",
      "f1 score: 0.492\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H          76   99      175             43.429\n",
      "A          58  101      159             63.522\n"
     ]
    }
   ],
   "source": [
    "labels = [\"H\", \"A\"]\n",
    "pos_label = \"H\"\n",
    "\n",
    "knn_accuracy = knn_correct / total_predicted\n",
    "knn_precision = myevaluation.binary_precision_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_recall = myevaluation.binary_recall_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_f1 = myevaluation.binary_f1_score(y_true, knn_preds, labels=labels, pos_label=pos_label)\n",
    "knn_confusion_matrix = myevaluation.confusion_matrix(y_true, knn_preds, labels)\n",
    "\n",
    "myutils.print_results(\"kNN Classifier\", knn_accuracy, knn_precision, knn_recall, knn_f1)\n",
    "header = [\"Winner\", \"H\", \"A\"]\n",
    "myutils.print_confusion_matrix(header, knn_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Once again, Naive Bayes performed the best of the classifiers with an accuracy of 61%. Precision, recall, and f1 were all also in the range of 60-65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Naive Bayes Classifier\n",
      "===========================================\n",
      "accuracy score: 0.587\n",
      "error rate: 0.413\n",
      "precision score: 0.603\n",
      "recall score: 0.617\n",
      "f1 score: 0.61\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         108   67      175             61.714\n",
      "A          71   88      159             55.346\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy = nb_correct / total_predicted\n",
    "nb_precision = myevaluation.binary_precision_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_recall = myevaluation.binary_recall_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_f1 = myevaluation.binary_f1_score(y_true, nb_preds, labels=labels, pos_label=pos_label)\n",
    "nb_confusion_matrix = myevaluation.confusion_matrix(y_true, nb_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Naive Bayes Classifier\", nb_accuracy, nb_precision, nb_recall, nb_f1)\n",
    "myutils.print_confusion_matrix(header, nb_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy\n",
    "Dummy performed with an accuracy of 47.3%. Due to the mechanics of Dummy, all performance stats are the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Dummy Classifier\n",
      "===========================================\n",
      "accuracy score: 0.473\n",
      "error rate: 0.527\n",
      "precision score: 0.498\n",
      "recall score: 0.857\n",
      "f1 score: 0.63\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H         150   25      175             85.714\n",
      "A         151    8      159              5.031\n"
     ]
    }
   ],
   "source": [
    "dummy_accuracy = dummy_correct / total_predicted\n",
    "dummy_precision = myevaluation.binary_precision_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_recall = myevaluation.binary_recall_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_f1 = myevaluation.binary_f1_score(y_true, dummy_preds, labels=labels, pos_label=pos_label)\n",
    "dummy_confusion_matrix = myevaluation.confusion_matrix(y_true, dummy_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Dummy Classifier\", dummy_accuracy, dummy_precision, dummy_recall, dummy_f1)\n",
    "myutils.print_confusion_matrix(header, dummy_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Decision tree surprisingly performed the worst, with an accuracy of 45.8%. This is likely due to overfitting. Pruning the decision tree could help performance (see below for pruning analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "Decision Tree Classifier\n",
      "===========================================\n",
      "accuracy score: 0.458\n",
      "error rate: 0.542\n",
      "precision score: 0.49\n",
      "recall score: 0.44\n",
      "f1 score: 0.464\n",
      "\n",
      "Winner      H    A    Total    Recognition (%)\n",
      "--------  ---  ---  -------  -----------------\n",
      "H          77   98      175             44\n",
      "A          80   79      159             49.686\n"
     ]
    }
   ],
   "source": [
    "dt_accuracy = dt_correct / total_predicted\n",
    "dt_precision = myevaluation.binary_precision_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_recall = myevaluation.binary_recall_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_f1 = myevaluation.binary_f1_score(y_true, dt_preds, labels=labels, pos_label=pos_label)\n",
    "dt_confusion_matrix = myevaluation.confusion_matrix(y_true, dt_preds, labels)\n",
    "\n",
    "myutils.print_results(\"Decision Tree Classifier\", dt_accuracy, dt_precision, dt_recall, dt_f1)\n",
    "myutils.print_confusion_matrix(header, dt_confusion_matrix, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Rules\n",
    "Below are the decison rules for the decision tree built using game statistics. Due to overfitting, the decision tree performed the worst of the classifiers. Pruning can help make the tree more generalizable without compromising the accuracy of the tree. Because the tree is already generated, pruning would be done with one of several post-pruning algorithms, such as reduced error, pessimistic error, minimum error, and error based pruning. Post-pruning looks for nodes in the tree with depth one subtrees, and if a pruning conditions, the subtree is replaced by an appropriate leaf node. Pruning condidtions look at the estimated error rates of each node to determine where pruning is neccessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF RegularLongestWStreak == A AND RegularLongestLStreak == A AND RegularEndingWStreak == A AND RegularTotalPlayed == A THEN ['Winner'] == H\n",
      "IF RegularLongestWStreak == A AND RegularLongestLStreak == A AND RegularEndingWStreak == A AND RegularTotalPlayed == H THEN ['Winner'] == A\n",
      "IF RegularLongestWStreak == A AND RegularLongestLStreak == A AND RegularEndingWStreak == H AND RegularTotalPlayed == A THEN ['Winner'] == H\n",
      "IF RegularLongestWStreak == A AND RegularLongestLStreak == A AND RegularEndingWStreak == H AND RegularTotalPlayed == H THEN ['Winner'] == A\n",
      "IF RegularLongestWStreak == A AND RegularLongestLStreak == H AND RegularTotalPlayed == A AND RegularEndingWStreak == A THEN ['Winner'] == H\n",
      "IF RegularLongestWStreak == A AND RegularLongestLStreak == H AND RegularTotalPlayed == A AND RegularEndingWStreak == H THEN ['Winner'] == A\n",
      "IF RegularLongestWStreak == A AND RegularLongestLStreak == H AND RegularTotalPlayed == H AND RegularEndingWStreak == A THEN ['Winner'] == H\n",
      "IF RegularLongestWStreak == A AND RegularLongestLStreak == H AND RegularTotalPlayed == H AND RegularEndingWStreak == H THEN ['Winner'] == H\n",
      "IF RegularLongestWStreak == H AND RegularLongestLStreak == A AND RegularEndingWStreak == A AND RegularTotalPlayed == A THEN ['Winner'] == A\n",
      "IF RegularLongestWStreak == H AND RegularLongestLStreak == A AND RegularEndingWStreak == A AND RegularTotalPlayed == H THEN ['Winner'] == A\n",
      "IF RegularLongestWStreak == H AND RegularLongestLStreak == A AND RegularEndingWStreak == H AND RegularTotalPlayed == A THEN ['Winner'] == A\n",
      "IF RegularLongestWStreak == H AND RegularLongestLStreak == A AND RegularEndingWStreak == H AND RegularTotalPlayed == H THEN ['Winner'] == H\n",
      "IF RegularLongestWStreak == H AND RegularLongestLStreak == H AND RegularTotalPlayed == A AND RegularEndingWStreak == A THEN ['Winner'] == A\n",
      "IF RegularLongestWStreak == H AND RegularLongestLStreak == H AND RegularTotalPlayed == A AND RegularEndingWStreak == H THEN ['Winner'] == A\n",
      "IF RegularLongestWStreak == H AND RegularLongestLStreak == H AND RegularTotalPlayed == H AND RegularEndingWStreak == A THEN ['Winner'] == H\n",
      "IF RegularLongestWStreak == H AND RegularLongestLStreak == H AND RegularTotalPlayed == H AND RegularEndingWStreak == H THEN ['Winner'] == H\n"
     ]
    }
   ],
   "source": [
    "attribute_names = table.column_names[3:-1]\n",
    "class_name = [\"Winner\"]\n",
    "dt.print_decision_rules(attribute_names, class_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
